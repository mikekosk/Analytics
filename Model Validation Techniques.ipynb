{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation \n",
    "\n",
    "In statistics and machine learning, model validation is the process of deciding whether the results of a model are sufficiently generalizable to describe and make predictions on similarly distributed data.\n",
    "\n",
    "Model validation largely revolves around the great dilemma between **bias** and **variance**.  Model developers want to choose a model that will accurately capture the predictions in the training data, but will also generalize well on unseen data.  \n",
    "\n",
    "- The first of these demands is a desire to reduce **bias**, which is the error from erroneous assumptions in the learning algorithm.  High bias models are lower accuracy because they do not utilize all the possible relations between features and predictions available in the data.  Low bias models will often be very complex, and will usually have a much higher accuracy as a result. The risk with low bias models is that they are *overfit* on the data and are not modelling on a *true* relationship between the features and predictions, but on the noise present in the data.  The opposite of overfitting is *underfitting*, and it is used to describe models that potentially miss a significant relationship between a feature and a prediction.\n",
    "\n",
    "\n",
    "- The second demand on a model developer is to reduce **variance**, which is the error from fluctuations in the underlying data.  High variance models are poor generalizers on data outside the training set.  Models that are (over)fit to the noise in the training data will not be able to make good predictions on data that has a different distribution. Low variance models should have reasonable accuracy out of sample, because they have correctly identified *real* relationships between the features and predictions, and not just noise.\n",
    "\n",
    "In the training phase of our model development process, care is taken to tune models such that they are able to minimize bias and variance as much as possible.  In this notebook, I'll be implementing the most basic of validation techniques, the **Test/Train Split** and **Cross-Validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "#Loading Necessary Background Packages\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train Splits\n",
    "\n",
    "If you're going to be developing any sort of predictive model, the minimal amount of validation will require splitting your data into testing and training datatsets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data \n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have split the dataset such that 20% of the data will be removed for testing purposes.  In this scenario, I have split the data such that the last 20% are reserved for testing (via random_state=False), but you can easily imagine that another method of splitting data might pull the 20% randomly from the dataset.  The use of this flag breaks down to whether or not we're dealing with time dependent data. \n",
    "\n",
    "If we're dealing with a time dimension in our data (e.g. financial analysis), it is more common to remove the last 6 months of data, for example, for testing purposes.  The reason being that it is easy to overfit data this way- You're still testing over points that match the general patterns in the training dataset.\n",
    "\n",
    "Below I'll be training the data on our training dataset, and measuring its fit on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model correctly classified 0.933333333333 of our test points\n"
     ]
    }
   ],
   "source": [
    "# load in SVC - a kernel classifier function from the scikit-learn library\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# create an instance of a kernel-based regressor from scikit learn\n",
    "classifier = SVC(kernel = 'rbf')\n",
    "\n",
    "# Fit on training dataset\n",
    "classifier.fit(X[train], y[train])\n",
    "\n",
    "# Print score\n",
    "print 'Our model correctly classified {} of our test points' \\\n",
    ".format(classifier.score(X[test], y[test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "Cross Validation is used in model development because it is one of the ways we get insights on out of sample model performance in addition to preventing overfitting of our models.  It is often done in addition to a hold out test set (as done above) and is often used to find the optimal hyperparameters for a given function.  \n",
    "\n",
    "Cross Validation techniques will generally split up the dataset in a way that either fixed numbers or portions of the dataset get iteratively placed into testing and training pools.  Two of the most common methods of cross validation are LeaveOneOut and K-Folds. \n",
    "\n",
    "#### K-Folds\n",
    "In K-Folds validation, the data is split into test/train datasets *k* times.  Each iteration of *k*-folds will test the data on 1/*k* of the dataset, while training it on the remaining *k*-1/*k* portion of the data.  Below is an illustration of the *k* iterations of *k*-folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://cse3521.artifice.cc/images/k-fold-cross-validation.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"http://cse3521.artifice.cc/images/k-fold-cross-validation.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-folds has been empircally shown to exhibit desireable tradeoffs between bias and variance with a *k* of 5 and 10, so you'll see these most commonly represented in tests and examples. [Citation needed]\n",
    "\n",
    "#### Leave One Out\n",
    "Leave one out is a special case of K-Folds whereby *k* is equivalent to the number of datapoints, *n*. \n",
    "\n",
    "#### Which is preferred?\n",
    "\n",
    "As you can imagine, leave one out can be quite computationally expensive-- each model is essentially run *n* times.  It is therefore usually only run on small datasets whereby the computational hit can be taken.  With regards to bias/variance, LOO CV will lead to estimates with lower bias, but higher variance because each training set will contain *n*-1 examples and will have overlap across training sets as you're using almost the entire training set in each iteration.  \n",
    "\n",
    "The opposite is true with k-fold CV, because there is relatively less overlap between training sets, thus the test error estimates are less correlated, as a result of which the mean test error value won't have as much variance as LOO CV.\n",
    "\n",
    "### Is cross validation a replacement for having a test data set?\n",
    "\n",
    "It is tempting to solely use cross validation in lieu of a test train split.  This is unadvised because while cross validation may provide some sense of out of sample model performance, it still uses the entirety of the training set to generate coefficients.  It is moot to compare models this way, because some models are simply much better at overfitting to the data given to them. \n",
    "\n",
    "We will be using the Iris dataset as above for classification purposes in this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import cross validation packages\n",
    "from sklearn.model_selection import KFold, LeaveOneOut\n",
    "\n",
    "# split into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cross validation packages will take the number of folds but otherwise are very simple to initialize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Building KFolds with 5 Folds\n",
    "KFold_5 = KFold(5)\n",
    "\n",
    "## Building KFolds with 10 Folds\n",
    "KFold_10 = KFold(10)\n",
    "\n",
    "## Building LOO, leaving one out (naturally) \n",
    "LooCV = LeaveOneOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the use of the cross validation packages, I will first initialize and display the prediction score of each fold of our 5-fold cross validation.\n",
    "\n",
    "Scikit-learn has a useful set of packages for iterating through each fold of a cross validation scheme in addition to making predictions on a model fit with cross validation.  Plenty of models have built in cross validation functions (ie: LassoCV, LogisticRegressionCV) but this is a model agnostic framework -- great for comparing the performance of a lot of models against eachother!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.83333333,  0.95833333,  1.        ,  1.        ,  0.875     ])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "\n",
    "# load in Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create an instance of Logistic Regression with default options\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Display the prediction score of each fold\n",
    "cross_val_score(classifier, X_train, y_train, cv=KFold_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see not every fold was predicted equally well in our cross validation scheme. \n",
    "\n",
    "We will use the cross_val_predict package to see how well our model works out of sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90000000000000002"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Predictions\n",
    "predicted = cross_val_predict(classifier, X_test, y_test)\n",
    "\n",
    "# Measure model performance\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Comparing performance of different models\n",
    "\n",
    "Below I'll be using our cross validation methodology (the k-fold with 5 folds) to test the out of sample performance of our models.  Each of the models will be fit on 4/5th of the training data 5 times and used to make 5 predictions on the out of sample data.  The 5 out of sample predictions will be averaged then to make the final prediction.\n",
    "\n",
    "I'm testing the performane of a random set of classifier models with relatively random hyperparameters.  If I wanted to get fancy I'd tune each of those hyperparameters individually, but as a start this is a good technique to see which models are looking promising on a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors 0.9\n",
      "Linear SVM 0.8\n",
      "RBF SVM 0.866666666667\n",
      "Gaussian Process 0.966666666667\n",
      "Decision Tree 0.9\n",
      "Random Forest 0.933333333333\n",
      "Neural Net 0.866666666667\n",
      "AdaBoost 0.9\n",
      "Naive Bayes 0.933333333333\n",
      "QDA 0.766666666667\n"
     ]
    }
   ],
   "source": [
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    predicted = cross_val_predict(clf, X_test, y_test, cv=KFold_5)\n",
    "    print name, metrics.accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Cross valiadation and the test/train split are the most basic methods of model validation you can do.  Bootstrapping and Bagging are two further techniques whereby models are tested on random samples of the training set, such that the distribution of the training set is varied.\n",
    "\n",
    "The success of ensemble meta-model techniques like Random Forests is largely because of the build in model validation that is used to create the final model.  To learn more about how these models improve the bias-variance tradeoff, see my post about ensemble models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
